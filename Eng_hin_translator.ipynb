{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55279bc8-fd89-4704-bb7b-9aa74ad93dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb76b462-7145-48c7-bec9-819fc270ff72",
   "metadata": {},
   "source": [
    "# Login to Hugging Face\n",
    "#### Before You can start using hugging face models and datasets, you need to Create a Token from the website. To generate token got to Settings > Access Tokens and generate and save the token with the required permissions. \n",
    "#### In the notebook run below code and provide the token that you created. Now you will be able to access the data and models present at hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ff15c6-13d2-4fba-96b2-a27d6243bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792bf51-62c5-48c1-9ade-79299e431126",
   "metadata": {},
   "source": [
    "# Import Dataset\n",
    "#### You can use load_dataset() to load datasets available on HuggingFace. It loads the data in a DatasetDict Format. When you print it you can see the meta data instead of the actual data. \n",
    "#### You can see that there is one key \"train\" which contains english_sentence and hindi_sentence which are our input text and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bd2299-4d91-4ee9-a56a-9c0529a3c1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english_sentence', 'hindi_sentence'],\n",
       "        num_rows: 127705\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_dataset = load_dataset(\"Aarif1430/english-to-hindi\")\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913879c4-013a-44f7-af7d-63cf644b8de2",
   "metadata": {},
   "source": [
    "#### You can look at the data by accessing the keys like a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b323d6ad-e5a5-43ac-b3ca-e6d13aff5c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english_sentence': 'politicians do not have permission to do what needs to be done.',\n",
       " 'hindi_sentence': '‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ú‡•ç‡§û‡•ã‡§Ç ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§ú‡•ã ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, ‡§µ‡§π ‡§ï‡§∞‡§®‡•á ‡§ï‡§ø ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à .'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409da492-6ac6-4c5f-9860-c9d1a5e286cd",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e09c52-6b2d-46c4-a9a1-4ed9d4b49986",
   "metadata": {},
   "source": [
    "#### First step is to convert your data into numbers which can be done using AutoTokenizer class.\n",
    "#### Almost all the models on hugging face have their own tokenizers that you can import using this library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30481e35-eb0f-4890-b65e-89414d1bef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18d7409a-36f5-4408-bb62-a1888fe62305",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"barghavani/English_to_Hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0f71ec1-64c5-458b-9910-6028c81b710e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [12110, 633, 300, 23, 34293, 363, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello My name is Priyam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae8f157-8e67-4480-8813-7f3255a6fba2",
   "metadata": {},
   "source": [
    "#### Here we are using preprocess function for comverting text to numbers\n",
    "#### At the end you will have the same imported DatasetDict with 2 added columns 1. input_ids (English sentences) 2. labels (Hindi Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "725d3b6d-bfb3-4cb4-a965-ec597c7c0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "def preprocess(example):\n",
    "    text = [en for en in example[\"english_sentence\"]]\n",
    "    labels = [hin for hin in example[\"hindi_sentence\"]]\n",
    "    model_input = tokenizer(text, max_length = max_length)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        label = tokenizer(labels, max_length = max_length)\n",
    "\n",
    "    model_input[\"labels\"] = label[\"input_ids\"]\n",
    "    return model_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1e1763c-dd5f-4a18-bfc4-d58521d562c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "D:\\Softwares\\Anaconda\\envs\\gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[21770, 110, 36, 55, 2961, 7, 110, 117, 1874, 7, 42, 846, 3, 0], [56, 70, 232, 288, 7, 1169, 27, 195, 131, 295, 1075, 2, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[40085, 86, 6, 173, 41, 358, 187, 236, 2, 49, 91, 25, 1611, 29, 5, 44, 3, 0], [6871, 383, 276, 58, 38, 929, 6, 207, 11, 8106, 44730, 3061, 2, 0]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = preprocess(raw_dataset[\"train\"][100:102])\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2894eb39-3af5-4d78-8b63-7b2f7f19d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = raw_dataset.map(preprocess, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41a329ca-6546-418b-afa4-d87b899e22b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english_sentence', 'hindi_sentence', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 127705\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e3f633-b567-448c-a31f-56d12120fa23",
   "metadata": {},
   "source": [
    "#### Splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84d27bc0-ac80-4a81-a148-a08c09480c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['english_sentence', 'hindi_sentence', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 114934\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tokenized_dataset['train'].train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c22e3663-399e-4c71-90af-60f9bc5816ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english_sentence', 'hindi_sentence', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 114934\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['english_sentence', 'hindi_sentence', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 12771\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b13f161-e060-4561-a968-afbe1e3f2d00",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d50107c-3c4f-4f49-b19c-678f85069f89",
   "metadata": {},
   "source": [
    "#### As our task is of Machine Translation which is a sequence to sequence task, we will use AutoModelForSeq2SeqLM. \n",
    "#### For different tasks you can use different such AutoModel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38218849-9a77-4df5-9d5b-88920fd1e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f9b8c7-68be-4ef6-90ab-1e9f9ac04a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"barghavani/English_to_Hindi\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c35ed-951f-433c-a9d2-2f5e61702772",
   "metadata": {},
   "source": [
    "# Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "058ba877-9933-4991-8100-dcb468c5f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 0.00003\n",
    "weight_decay = 0.01\n",
    "number_of_train_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755108b1-641d-4fd9-bc51-50db2c465020",
   "metadata": {},
   "source": [
    "#### Data collator is used for supplying our input data in batches to avoid any memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8576f082-3c14-42b4-aea2-e2d5cbf30921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ec4c22d-9c2a-4652-aae3-7fc5b87b78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors = \"pt\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87cfc5b8-781c-47eb-b489-a158bca50a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda\\envs\\gpu\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd9e7caa-3f11-4332-8816-58d6739391a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['english_sentence', 'hindi_sentence', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 114934\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b05d4-1fd4-4bcf-8935-6e2320298684",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ee6227d-ef77-4521-afd5-48c13aec4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba8c425-5072-4edf-ab7f-1ea2b0c3e042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3486' max='7184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3486/7184 4:23:59 < 4:40:12, 0.22 it/s, Epoch 0.49/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36774365-ddef-4ae4-96d4-a9f59ad60a74",
   "metadata": {},
   "source": [
    "# Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26e37cbe-92ac-40a5-9637-a4fc2382cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    \n",
    "    # Generate translation\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode the generated sequences\n",
    "    translated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e30e9b2-96f6-4e25-a819-8a0b5df10abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: ‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§ó‡§æ‡§Ø‡§ï ‡§π‡•Ç‡§Å ‡§ú‡•ã ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§§‡§∞‡§π ‡§∏‡•á ‡§®‡•É‡§§‡•ç‡§Ø ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç ‡§≤‡•á‡§ï‡§ø‡§® ‡§¨‡•Å‡§∞‡§æ ‡§ó‡§æ‡§§‡§æ ‡§π‡•à\n"
     ]
    }
   ],
   "source": [
    "input_text = \"i am a singer who dances well but sings bad\"\n",
    "translated_text = translate(input_text)\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ee37b-86b4-4673-a4fc-21d029f062ad",
   "metadata": {},
   "source": [
    "# Save Fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c92f21-e03e-4b42-a0e5-9f4bc3df10f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where you want to save the model\n",
    "save_directory = \"./fine_tuned_model\"\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a970a-d7bc-4480-a14d-415ccf8ba767",
   "metadata": {},
   "source": [
    "# Load Fine Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b06a3671-76d1-4f07-9408-c5a167d1fe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "save_directory = r\"D:\\courses\\NLP\\Transformers\\results\\checkpoint-3000\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c001d-fd20-42f0-a663-b15802d135ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
